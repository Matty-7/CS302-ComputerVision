{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook draws heavily from Francois Chollets Keras tutorial \n",
    "and Google LLC's cats versus dogs classifier tutorial.\n",
    "\n",
    "The raw data is a reduced set of the Kaggle Dogs vs Cats dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 20\n",
    "plt.rcParams['xtick.labelsize'] = 16\n",
    "plt.rcParams['ytick.labelsize'] = 16\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the path to the different directories\n",
    "base_dir = './'                   # adapt if you store the images not in the same directory\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "\n",
    "# Directory with our training cat pictures\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "\n",
    "# Directory with our training dog pictures\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "\n",
    "# Directory with our validation cat pictures\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "\n",
    "# Directory with our validation dog pictures\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
    "\n",
    "# get the names of the training images\n",
    "train_cat_fnames = os.listdir(train_cats_dir)\n",
    "train_dog_fnames = os.listdir(train_dogs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total training cat images:', len(os.listdir(train_cats_dir)))\n",
    "print('total training dog images:', len(os.listdir(train_dogs_dir)))\n",
    "print('total validation cat images:', len(os.listdir(validation_cats_dir)))\n",
    "print('total validation dog images:', len(os.listdir(validation_dogs_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show some of the images\n",
    "nrows = 2\n",
    "ncols = 4\n",
    "\n",
    "plt.figure(figsize=(ncols * 4, nrows * 4))\n",
    "\n",
    "# replace cat with dog in the next two lines to see dog images\n",
    "next_pix = [os.path.join(train_cats_dir, fname) \n",
    "                for fname in train_cat_fnames[0:8]]\n",
    "\n",
    "for i, img_path in enumerate(next_pix):\n",
    "    \n",
    "    plt.subplot(nrows, ncols, i+1)\n",
    "    img = mpimg.imread(img_path)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) we learn how to load images on the fly from the hard disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# All gray values will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Flow training images in batches of 20 using train_datagen generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,  # This is the source directory for training images\n",
    "    \n",
    "        target_size=(150, 150),  # All images will be resized to 150x150\n",
    "    \n",
    "        batch_size=20,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')\n",
    "\n",
    "# Flow validation images in batches of 20 using val_datagen generator\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a simple CNN using \n",
    "\n",
    "# Our input feature map is 150x150x3: 150x150 for the image pixels, and 3 for\n",
    "# the three color channels: R, G, and B\n",
    "img_input = layers.Input(shape=(150, 150, 3))\n",
    "\n",
    "# First convolution creates 16 feature maps using a 3x3 filter\n",
    "# Convolution is followed by max-pooling layer with a 2x2 window\n",
    "x = layers.Conv2D(16, 3, activation='relu')(img_input)\n",
    "x = layers.MaxPooling2D(2)(x)\n",
    "\n",
    "# Second convolution creates 32 feature maps using a 3x3 filter\n",
    "# Convolution is followed by max-pooling layer with a 2x2 window\n",
    "x = layers.Conv2D(32, 3, activation='relu')(x)\n",
    "x = layers.MaxPooling2D(2)(x)\n",
    "\n",
    "# Third convolution creates 64 feature maps using a 3x3 filter\n",
    "# Convolution is followed by max-pooling layer with a 2x2 window\n",
    "x = layers.Conv2D(64, 3, activation='relu')(x)\n",
    "x = layers.MaxPooling2D(2)(x)\n",
    "\n",
    "#------------------------------\n",
    "# Flatten feature map to a 1-dim tensor so we can add fully connected layers\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "# Create a fully connected layer with ReLU activation and 512 hidden units\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "\n",
    "# Create output layer with a single node and sigmoid activation\n",
    "output = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Create model:\n",
    "model = Model(img_input, output)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=RMSprop(learning_rate=0.001),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('initial_weights.h5')\n",
    "history = model.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,  # 2000 images = batch_size * steps\n",
    "      epochs=10,\n",
    "      validation_data=validation_generator,\n",
    "      #validation_steps=20,  # useful if your validation data do not fit in memory\n",
    "      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = history.history['acc']\n",
    "val_accuracy = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1,len(accuracy)+1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(11,7))\n",
    "plt.plot(epochs, accuracy, 'bo', label='Training')\n",
    "plt.plot(epochs, val_accuracy, 'r', label='Test')\n",
    "#plt.xticks(np.arange(0, 21, step=4))  # Set label locations.\n",
    "plt.xlabel('Epoch', size=18)\n",
    "plt.ylabel('Accuracy', size=18)\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(11,7))\n",
    "plt.plot(epochs, loss, 'bo', label='Training')\n",
    "plt.plot(epochs, val_loss, 'r', label='Test')\n",
    "#plt.xticks(np.arange(0, 21, step=4))  # Set label locations.\n",
    "plt.xlabel('Epoch', size=18)\n",
    "plt.ylabel('Loss', size=18)\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, we have massive overfitting. Which was to be expected for a network with almost 9.5 million parameters trained on 2000 images only. Let's increase our training dataset with image augmentation.\n",
    "\n",
    "# => back to slides "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) we reduce overfitting with image augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we add shearing, zooming, and horizontal flips to the images\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "# Flow training images in batches of 20 using train_datagen generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,  \n",
    "        target_size=(150, 150),  \n",
    "        batch_size=20,\n",
    "        class_mode='binary')\n",
    "\n",
    "\n",
    "# Flow validation images in batches of 20 using val_datagen generator\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('initial_weights.h5') # make sure we start from the same pristine model\n",
    "\n",
    "history_aug = model.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,  # 2000 images = batch_size * steps\n",
    "      epochs=10,\n",
    "      validation_data=validation_generator,\n",
    "      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = history_aug.history['acc']\n",
    "val_accuracy = history_aug.history['val_acc']\n",
    "loss = history_aug.history['loss']\n",
    "val_loss = history_aug.history['val_loss']\n",
    "epochs = range(1,len(accuracy)+1)\n",
    "\n",
    "plt.figure(figsize=(11,7))\n",
    "plt.plot(epochs, accuracy, 'bo', label='Training')\n",
    "plt.plot(epochs, val_accuracy, 'r', label='Test')\n",
    "#plt.xticks(np.arange(0, 21, step=4))  # Set label locations.\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(11,7))\n",
    "plt.plot(epochs, loss, 'bo', label='Training')\n",
    "plt.plot(epochs, val_loss, 'r', label='Test')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the result: improved accuracy, reduced overfitting \n",
    "\n",
    "# => back to slides "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) we reduce overfitting using transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "n_classes =2\n",
    "\n",
    "# load the model \n",
    "base_model = keras.applications.xception.Xception(weights=\"imagenet\",\n",
    "                                                    include_top=False)   # but not the dense layer at the output\n",
    "#create our own output model\n",
    "avg = layers.GlobalAveragePooling2D()(base_model.output)                  \n",
    "output = layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
    "\n",
    "# combine the two parts\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will not train the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False                \n",
    "\n",
    "    \n",
    "# we train our dense output layer    \n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.2, momentum=0.9)\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,  # 2000 images = batch_size * steps\n",
    "      epochs=1,\n",
    "      validation_data=validation_generator,\n",
    "      verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) let's make predictions for 20 images from the predict dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first show the images\n",
    "predict_dir = os.path.join(base_dir, 'predict')\n",
    "predict_images_dir = os.path.join(predict_dir, 'images')\n",
    "predict_fnames = sorted(os.listdir(predict_images_dir)) # sorted is important because flow_from_directory does so too\n",
    "\n",
    "nrows = 4\n",
    "ncols = 5\n",
    "\n",
    "next_pix = [os.path.join(predict_images_dir, fname) \n",
    "              for fname in predict_fnames[0:ncols*nrows]]\n",
    "\n",
    "plt.figure(figsize=(ncols * 4, nrows * 4))\n",
    "for i, img_path in enumerate(next_pix):    \n",
    "    plt.subplot(nrows, ncols, i+1)\n",
    "    img = mpimg.imread(img_path)\n",
    "    plt.title(predict_fnames[i])\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second: make the predictions\n",
    "image_generator = test_datagen.flow_from_directory(\n",
    "        predict_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='binary',\n",
    "        shuffle=False)               # important to keep the filenames and results aligned\n",
    "\n",
    "\n",
    "y_pred = model.predict(image_generator)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create labels\n",
    "y_pred=np.round(y_pred[:,0])\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
